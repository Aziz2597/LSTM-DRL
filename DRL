
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import load_model
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
import gym
from gym import spaces
import matplotlib.pyplot as plt

class PortfolioTradingEnv(gym.Env):
    def __init__(self, df, lstm_model, window_size=10):
        self.df = df
        self.lstm_model = lstm_model
        self.window_size = window_size
        self.current_step = window_size
        self.max_steps = len(df) - window_size - 1
        
        # Action space: [buy/sell/hold] + position size
        self.action_space = spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32)
        
        # Observation space: market state + LSTM prediction + portfolio state
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, 
            shape=(window_size, len(columns) + 3),  # Original features + 2 new
            dtype=np.float32
        )
        
        # Portfolio parameters
        self.initial_balance = 1e6  # ₹1,000,000
        self.balance = self.initial_balance
        self.portfolio_value = []
        self.positions = []

    def reset(self):
        self.current_step = self.window_size
        self.balance = self.initial_balance
        self.portfolio_value = [self.initial_balance]
        self.positions = []
        return self._get_observation()
    
    def _get_observation(self):
        # Get current window data
        window_data = data_scaled[self.current_step-self.window_size:self.current_step]
        
        # Get LSTM prediction for next period
        lstm_input = window_data.reshape(1, self.window_size, -1)
        predicted_price = self.lstm_model.predict(lstm_input, verbose=0)[0][0]
        predicted_price = self.lstm_model.predict(lstm_input, verbose=0)[0][0]
        if np.isnan(predicted_price):
           predicted_price = 0.0

# After enhanced_window
        if np.isnan(enhanced_window).any():
           print(f"NaNs in observation at step {self.current_step}")
           enhanced_window = np.nan_to_num(enhanced_window)

        
        # Combine with portfolio state
        portfolio_state = np.array([
            self.balance / self.initial_balance,
            len(self.positions) > 0 and self.positions[-1] or 0
        ])
        
        # Add LSTM prediction as new feature
        enhanced_window = np.concatenate([
            window_data, 
            np.full((self.window_size, 1), predicted_price),
            np.tile(portfolio_state, (self.window_size, 1))
        ], axis=1)
        
        return enhanced_window

    def step(self, action):
        # Execute trade
        current_price = df['Open Price'].iloc[self.current_step]
        action_type = action[0]  # [-1, 1] (sell to buy)
        position_size = abs(action[1])  # [0, 1] (percentage of portfolio)
        
        # Calculate transaction
        transaction_value = position_size * self.balance
        transaction_cost = transaction_value * 0.001  # 0.1% fee
        
        if action_type > 0:  # Buy
            shares = transaction_value / current_price
            self.balance -= (transaction_value + transaction_cost)
            self.positions.append(shares)
        elif action_type < 0:  # Sell
            if len(self.positions) > 0:
                shares = min(position_size * sum(self.positions), sum(self.positions))
                sale_value = shares * current_price
                self.balance += (sale_value - transaction_cost)
                self.positions = [p - shares for p in self.positions]

        # Update portfolio value
        position_value = sum(self.positions) * current_price if self.positions else 0
        total_value = self.balance + position_value
        self.portfolio_value.append(total_value)
        
        # Calculate reward (Sharpe Ratio)
        returns = np.diff(self.portfolio_value) / self.portfolio_value[:-1]
        if len(returns) == 0 or np.std(returns) == 0:
             sharpe = 0.0
        else:
             sharpe = np.mean(returns) / (np.std(returns) + 1e-9)

# Also prevent negative/overflow portfolio value:
        if not np.isfinite(total_value):
            total_value = self.initial_balance

        sharpe = np.mean(returns) / (np.std(returns) + 1e-9)
        
        # Prepare next state
        self.current_step += 1
        done = self.current_step >= self.max_steps
        
        return self._get_observation(), sharpe, done, {}

# Load the pre-trained LSTM model
lstm_model = load_model('your_lstm_model.h5', compile=False)


# Create environment
env = DummyVecEnv([lambda: PortfolioTradingEnv(df, lstm_model)])

# Train DRL agent
model = PPO(
    "MlpPolicy",
    env,
    verbose=1,
    learning_rate=3e-4,
    n_steps=2048,
    batch_size=128,
    n_epochs=10,
    gamma=0.99,
    gae_lambda=0.95,
    clip_range=0.2,
    tensorboard_log="./portfolio_tb/"
)

print("Starting DRL training...")
model.learn(total_timesteps=100000)
model.save("portfolio_drl_model")

# Evaluation
def evaluate_model(model, env):
    obs = env.reset()
    done = False
    while not done:
        action, _ = model.predict(obs)
        obs, _, done, _ = env.step(action)
    
    portfolio = env.envs[0].portfolio_value
    returns = np.diff(portfolio) / portfolio[:-1]
    
    print(f"\nFinal Portfolio Value: ₹{portfolio[-1]:,.2f}")
    print(f"Sharpe Ratio: {np.mean(returns)/np.std(returns):.2f}")
    print(f"Max Drawdown: {(np.max(portfolio) - np.min(portfolio))/np.max(portfolio):.2%}")
    
    plt.figure(figsize=(12,6))
    plt.plot(portfolio)
    plt.title("Portfolio Value Over Time")
    plt.xlabel("Trading Days")
    plt.ylabel("Portfolio Value (₹)")
    plt.show()

print("\nEvaluating trained model...")
evaluate_model(model, env)
